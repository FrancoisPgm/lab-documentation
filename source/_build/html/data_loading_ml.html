

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Loading big-data for machine learning &mdash; simexp-documentation 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> simexp-documentation
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Loading big-data for machine learning</a><ul>
<li><a class="reference internal" href="#keras-batch-loading">Keras batch loading</a></li>
<li><a class="reference internal" href="#tips">TIPS</a></li>
</ul>
</li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">simexp-documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Loading big-data for machine learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/data_loading_ml.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="loading-big-data-for-machine-learning">
<h1>Loading big-data for machine learning<a class="headerlink" href="#loading-big-data-for-machine-learning" title="Permalink to this headline">¶</a></h1>
<p>In functionnal MRI, it is pretty common to deal with huge databases.
For example, <a class="reference external" href="https://www.ukbiobank.ac.uk/">uk biobank</a> contains thousands of experimentation where:
each experimentation has a certain number of patients -&gt; each patient has few bolds scans -&gt; one fmri scan is about ~ <span class="math notranslate nohighlight">\(200\)</span> MB.</p>
<p>If you want to fit all the data into the memory, you would need terrabytes of RAM!
This is not feasible nowadays (though it is evolving using <a class="reference external" href="https://searchstorage.techtarget.com/definition/PCIe-SSD-PCIe-solid-state-drive">NVMe PCIe SSD as RAM</a>).</p>
<p>In this tutorial, we will show how to load and train an entire dataset.</p>
<div class="section" id="keras-batch-loading">
<h2>Keras batch loading<a class="headerlink" href="#keras-batch-loading" title="Permalink to this headline">¶</a></h2>
<p>Typically in deep learning, you will use SGD to train your model batch per batch. The trick to manage this amount of data is to load the batch data on the fly, during the training. This can be done by using the (fit_generator)[<a class="reference external" href="https://keras.io/models/model/#fit_generator">https://keras.io/models/model/#fit_generator</a>] from keras, and a data generator (the object keras.util.sequence)</p>
<p>So now let’s say you have a batch of 32 nifti files which is ~1G in RAM. Reading the nifti image one by one inside a loop, without any optimization can make your data generator longer than the fitting ! This can be seen with tensorflow for example, where sometimes you would use 1 cpu (for the data loading) and all the cpus for the fitting. Or the GPU would be used just 10% of the time, because your data_loading takes 90% to do. <a class="reference external" href="https://blog.sopticek.net/2017/06/03/concurrent-and-parallel-programming-in-python-part-2/">https://blog.sopticek.net/2017/06/03/concurrent-and-parallel-programming-in-python-part-2/</a> <a class="reference external" href="https://aws.amazon.com/blogs/machine-learning/parallelizing-across-multiple-cpu-gpus-to-speed-up-deep-learning-inference-at-the-edge/">https://aws.amazon.com/blogs/machine-learning/parallelizing-across-multiple-cpu-gpus-to-speed-up-deep-learning-inference-at-the-edge/</a></p>
<p>thread vs multiprocesses: I/O bound (downloads, read/write disk, ) =&gt; threads cpu bound (data management, computation) =&gt; multiprocessing <a class="reference external" href="https://www.quantstart.com/articles/Parallelising-Python-with-Threading-and-Multiprocessing">https://www.quantstart.com/articles/Parallelising-Python-with-Threading-and-Multiprocessing</a></p>
<p>pool vs process : for huge amount of processes (&gt; thousands) and low I/O, use multiprocessing.Pool which can efficiently dispatch your tasks among the available workers. If you have slow number of process (~tens) and intense I/O, then use multiprocessing.Process(). <a class="reference external" href="https://medium.com/datadriveninvestor/python-multiprocessing-pool-vs-process-comparative-analysis-6c03c5b54eec">https://medium.com/datadriveninvestor/python-multiprocessing-pool-vs-process-comparative-analysis-6c03c5b54eec</a> <a class="reference external" href="https://www.ellicium.com/python-multiprocessing-pool-process/">https://www.ellicium.com/python-multiprocessing-pool-process/</a></p>
<p>good overview: <a class="reference external" href="http://blog.shenwei.me/python-multiprocessing-pool-difference-between-map-apply-map_async-apply_async/">http://blog.shenwei.me/python-multiprocessing-pool-difference-between-map-apply-map_async-apply_async/</a></p>
<p>Data feeder example in DL: <a class="reference external" href="https://github.com/asheshjain399/Tensormodels/blob/master/tensormodels/data_reader/data_feeder.py">https://github.com/asheshjain399/Tensormodels/blob/master/tensormodels/data_reader/data_feeder.py</a></p>
</div>
<div class="section" id="tips">
<h2>TIPS<a class="headerlink" href="#tips" title="Permalink to this headline">¶</a></h2>
<p>To reduce the RAM usage, change the data type to <code class="docutils literal notranslate"><span class="pre">float32</span></code> instead of <code class="docutils literal notranslate"><span class="pre">float64</span></code>.
Indeed, the precision loss in machine learning is ridiculous compared to the time gain.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">/scratch</span></code> space when you are working on an HPC. You can expect up to <span class="math notranslate nohighlight">\(10\)</span> % speed improvement when loading the batches.</p>
<p>If you are using a gpu, you can load the data before fitting.
Simply use the argument <code class="code docutils literal notranslate"><span class="pre">use_multiprocessing=True</span></code> with <a class="reference external" href="https://keras.io/models/sequential/">keras.fit_generator</a>.</p>
<p>use multiprocesing when loading your batch (one process per sample)</p>
<p>remove loops and use <a href="#id1"><span class="problematic" id="id2">`SIMD&lt;https://towardsdatascience.com/decoding-the-performance-secret-of-worlds-most-popular-data-science-library-numpy-7a7da54b7d72&gt;`_</span></a> instructions</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, SIMEXP Pierre Bellec

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>